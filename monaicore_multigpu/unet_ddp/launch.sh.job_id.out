/home/hju/monai_uf_tutorials/monaicore_multigpu/unet_ddp
c1100a-s35.ufhpc
Fri Aug 12 23:17:48 EDT 2022
Primary node: c1100a-s35
Primary TCP port: 28664
Secondary nodes: 
Running "/home/hju/monai_uf_tutorials/monaicore_multigpu/unet_ddp/unet_training_ddp.py" on each node...
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (484) bind mounts
/.singularity.d/env/10-docker2singularity.sh: line 2: uname: No such file or directory
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[0] generating synthetic data to ./testdata (this may take a while)
NCCL version 2.11.4+cuda11.4
[0] ---------- epoch 1/5
[1] ---------- epoch 1/5[2] ---------- epoch 1/5[3] ---------- epoch 1/5


[2] 1/32, train_loss: 0.7367
[1] 1/32, train_loss: 0.6082[3] 1/32, train_loss: 0.6416

[0] 1/32, train_loss: 0.6633
[1] 2/32, train_loss: 0.5802
[0] 2/32, train_loss: 0.5791
[3] 2/32, train_loss: 0.5304
[2] 2/32, train_loss: 0.6746
[1] 3/32, train_loss: 0.5242
[3] 3/32, train_loss: 0.5739
[2] 3/32, train_loss: 0.5711
[0] 3/32, train_loss: 0.5127
[2] 4/32, train_loss: 0.5749
[3] 4/32, train_loss: 0.5164
[1] 4/32, train_loss: 0.5557
[0] 4/32, train_loss: 0.5476
[3] 5/32, train_loss: 0.6410
[2] 5/32, train_loss: 0.4873
[0] 5/32, train_loss: 0.5426
[1] 5/32, train_loss: 0.5604
[3] 6/32, train_loss: 0.5183
[0] 6/32, train_loss: 0.4900
[1] 6/32, train_loss: 0.5096
[2] 6/32, train_loss: 0.4665
[3] 7/32, train_loss: 0.5295
[0] 7/32, train_loss: 0.4988
[2] 7/32, train_loss: 0.4984
[1] 7/32, train_loss: 0.5120
[3] 8/32, train_loss: 0.5219
[2] 8/32, train_loss: 0.6013
[0] 8/32, train_loss: 0.4877
[1] 8/32, train_loss: 0.5457
[0] epoch 1, average loss: 0.5402
[0] ---------- epoch 2/5
[3] epoch 1, average loss: 0.5591
[3] ---------- epoch 2/5
[1] epoch 1, average loss: 0.5495
[1] ---------- epoch 2/5
[2] epoch 1, average loss: 0.5763
[2] ---------- epoch 2/5
[0] 1/32, train_loss: 0.5194
[2] 1/32, train_loss: 0.5260
[1] 1/32, train_loss: 0.5465
[3] 1/32, train_loss: 0.5936
[2] 2/32, train_loss: 0.5490
[3] 2/32, train_loss: 0.5326
[1] 2/32, train_loss: 0.5004
[0] 2/32, train_loss: 0.4737
[1] 3/32, train_loss: 0.4555
[2] 3/32, train_loss: 0.4722
[3] 3/32, train_loss: 0.5626
[0] 3/32, train_loss: 0.5061
[0] 4/32, train_loss: 0.4664
[1] 4/32, train_loss: 0.5115
[2] 4/32, train_loss: 0.5599
[3] 4/32, train_loss: 0.5039
[0] 5/32, train_loss: 0.4905
[1] 5/32, train_loss: 0.5062
[2] 5/32, train_loss: 0.4920
[3] 5/32, train_loss: 0.5958
[2] 6/32, train_loss: 0.4952
[1] 6/32, train_loss: 0.4990[0] 6/32, train_loss: 0.5197

[3] 6/32, train_loss: 0.4254
[2] 7/32, train_loss: 0.5577
[0] 7/32, train_loss: 0.4933
[1] 7/32, train_loss: 0.4749
[3] 7/32, train_loss: 0.4433
[2] 8/32, train_loss: 0.5116
[3] 8/32, train_loss: 0.4209
[1] 8/32, train_loss: 0.4082
[0] 8/32, train_loss: 0.4605
[1] epoch 2, average loss: 0.4878
[1] ---------- epoch 3/5
[2] epoch 2, average loss: 0.5204
[2] ---------- epoch 3/5
[0] epoch 2, average loss: 0.4912
[0] ---------- epoch 3/5
[3] epoch 2, average loss: 0.5098
[3] ---------- epoch 3/5
[1] 1/32, train_loss: 0.4549
[0] 1/32, train_loss: 0.4757
[2] 1/32, train_loss: 0.5381
[3] 1/32, train_loss: 0.5369
[1] 2/32, train_loss: 0.4027
[0] 2/32, train_loss: 0.4971
[2] 2/32, train_loss: 0.5339
[3] 2/32, train_loss: 0.4485
[0] 3/32, train_loss: 0.5061
[1] 3/32, train_loss: 0.5422
[2] 3/32, train_loss: 0.4764
[3] 3/32, train_loss: 0.4231
[1] 4/32, train_loss: 0.4671
[0] 4/32, train_loss: 0.4851
[3] 4/32, train_loss: 0.4445
[2] 4/32, train_loss: 0.4939
[2] 5/32, train_loss: 0.5482
[0] 5/32, train_loss: 0.4423
[1] 5/32, train_loss: 0.5062
[3] 5/32, train_loss: 0.4497
[1] 6/32, train_loss: 0.4255
[2] 6/32, train_loss: 0.4109
[3] 6/32, train_loss: 0.4498
[0] 6/32, train_loss: 0.4725
[1] 7/32, train_loss: 0.5748
[2] 7/32, train_loss: 0.4334
[0] 7/32, train_loss: 0.4375
[3] 7/32, train_loss: 0.4569
[2] 8/32, train_loss: 0.4732
[3] 8/32, train_loss: 0.4781
[1] 8/32, train_loss: 0.5551
[0] 8/32, train_loss: 0.4376
[2] epoch 3, average loss: 0.4885
[2] ---------- epoch 4/5
[0] epoch 3, average loss: 0.4692
[0] ---------- epoch 4/5
[1] epoch 3, average loss: 0.4910
[1] ---------- epoch 4/5
[3] epoch 3, average loss: 0.4609
[3] ---------- epoch 4/5
[1] 1/32, train_loss: 0.4442
[2] 1/32, train_loss: 0.5928
[0] 1/32, train_loss: 0.4693
[3] 1/32, train_loss: 0.4213
[2] 2/32, train_loss: 0.3796
[1] 2/32, train_loss: 0.4378
[3] 2/32, train_loss: 0.4614
[0] 2/32, train_loss: 0.5699
[1] 3/32, train_loss: 0.4353
[2] 3/32, train_loss: 0.4017[0] 3/32, train_loss: 0.4581

[3] 3/32, train_loss: 0.6294
[0] 4/32, train_loss: 0.3461
[2] 4/32, train_loss: 0.4551
[1] 4/32, train_loss: 0.4453
[3] 4/32, train_loss: 0.4886
[2] 5/32, train_loss: 0.4384
[0] 5/32, train_loss: 0.4499
[1] 5/32, train_loss: 0.5035
[3] 5/32, train_loss: 0.4008
[0] 6/32, train_loss: 0.4491
[2] 6/32, train_loss: 0.4512
[1] 6/32, train_loss: 0.4850
[3] 6/32, train_loss: 0.4119
[2] 7/32, train_loss: 0.3800
[0] 7/32, train_loss: 0.4237
[1] 7/32, train_loss: 0.4958
[3] 7/32, train_loss: 0.4585
[0] 8/32, train_loss: 0.5237
[2] 8/32, train_loss: 0.4630
[1] 8/32, train_loss: 0.4416
[3] 8/32, train_loss: 0.4567
[0] epoch 4, average loss: 0.4612
[0] ---------- epoch 5/5
[1] epoch 4, average loss: 0.4611
[1] ---------- epoch 5/5
[3] epoch 4, average loss: 0.4661
[3] ---------- epoch 5/5
[2] epoch 4, average loss: 0.4452
[2] ---------- epoch 5/5
[3] 1/32, train_loss: 0.4250
[1] 1/32, train_loss: 0.5373
[2] 1/32, train_loss: 0.4369
[0] 1/32, train_loss: 0.5266
[1] 2/32, train_loss: 0.4449
[0] 2/32, train_loss: 0.3782
[2] 2/32, train_loss: 0.4306
[3] 2/32, train_loss: 0.5872
[1] 3/32, train_loss: 0.3881
[3] 3/32, train_loss: 0.3806
[0] 3/32, train_loss: 0.4945
[2] 3/32, train_loss: 0.4231
[3] 4/32, train_loss: 0.4335
[1] 4/32, train_loss: 0.4509
[2] 4/32, train_loss: 0.3987
[0] 4/32, train_loss: 0.4226
[2] 5/32, train_loss: 0.4511
[0] 5/32, train_loss: 0.4153
[3] 5/32, train_loss: 0.4222
[1] 5/32, train_loss: 0.5038
[0] 6/32, train_loss: 0.4385
[1] 6/32, train_loss: 0.4958[2] 6/32, train_loss: 0.3834

[3] 6/32, train_loss: 0.3997
[0] 7/32, train_loss: 0.3872
[2] 7/32, train_loss: 0.4758
[1] 7/32, train_loss: 0.4557
[3] 7/32, train_loss: 0.4112
[0] 8/32, train_loss: 0.4895
[2] 8/32, train_loss: 0.4722
[1] 8/32, train_loss: 0.4525
[3] 8/32, train_loss: 0.3917
[2] epoch 5, average loss: 0.4340
[2] train completed, epoch losses: [0.5763300582766533, 0.5204374231398106, 0.4884908050298691, 0.44522930309176445, 0.4339669346809387]
[0] epoch 5, average loss: 0.4441
[0] train completed, epoch losses: [0.5402176454663277, 0.49120576307177544, 0.4692375101149082, 0.46121516451239586, 0.4440634250640869]
[1] epoch 5, average loss: 0.4662
[1] train completed, epoch losses: [0.5494939610362053, 0.4877799153327942, 0.4910440221428871, 0.4610709324479103, 0.4661533534526825]
[3] epoch 5, average loss: 0.4314
[3] train completed, epoch losses: [0.5591176077723503, 0.5097758136689663, 0.4609384350478649, 0.46607349067926407, 0.43139681965112686]
