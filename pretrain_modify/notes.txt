[ok] compare with BRATS
[ok] compare with original,
- print in each process 
- add original code, 
- add comments in notes.txt, 
- every used script
[ok] move data, .json on hpg
[ok] single GPU
[] multi GPU
    - [ok] vanilla
    - [not work] cachedataset
        --mem=200G, load 47%, fails
        --mem=600G, take ~50min to load, hangs at
        `while loop: new epoch, global_step 0 -------------------------`

    - [not work] smartdataset
        `[0] train: epoch 0/2, step_within_epoch 0/0`, 
        num_iter/epoch is 0, len(train_loader) - 1 = 0 
        8 GPUs, batch_size = 1, 722/8 = 90 iters/epoch

    - [ok] resume
        - [ok] from theirs, fixed in code
        RuntimeError: Error(s) in loading state_dict for SSLHead:
	    Missing key(s) in state_dict: "swinViT.patch_embed.proj.weight", "swinViT.patch_embed.proj.bias", "swinViT.layers1.0.blocks.0.norm1.weight", "swinViT.layers1.0.blocks.0.norm1.bias", "swinViT.layers1.0.blocks.0.attn.relative_position_bias_table", "swinViT.layers1.0.blocks.0.attn.relative_position_index", "swinViT.layers1.0.blocks.0.attn.qkv.weight", "swinViT.layers1.0.blocks.0.attn.qkv.bias", "swinViT.layers1.0.blocks.0.attn.proj.weight", "swinViT.layers1.0.blocks.0.attn.proj.bias", "swinViT.layers1.0.blocks.0.norm2.weight", "swinViT.layers1.0.blocks.0.norm2.bias", "swinViT.layers1.0.blocks.0.mlp.linear1.weight", "swinViT.layers1.0.blocks.0.mlp.linear1.bias", "swinViT.layers1.0.blocks.0.mlp.linear2.weight", "swinViT.layers1.0.blocks.0.mlp.linear2.bias", "swinViT.layers1.0.blocks.1.norm1.weight", "swinViT.layers1.0.blocks.1.norm1.bias", "swinViT.layers1.0.blocks.1.attn.relative_position_bias_table", "swinViT.layers1.0.blocks.1.attn.relative_position_index", "swinViT.layers1.0.blocks.1.attn.qkv.weight", "swinViT.layers1.0.blocks.1.attn.qkv.bias", "swinViT.layers1.0.blocks.1.attn.proj.weight", "swinViT.layers1.0.blocks.1.attn.proj.bias", "swinViT.layers1.0.blocks.1.norm2.weight", "swinViT.layers1.0.blocks.1.norm2.bias", "swinViT.layers1.0.blocks.1.mlp.linear1.weight", "swinViT.layers1.0.blocks.1.mlp.linear1.bias", "swinViT.layers1.0.blocks.1.mlp.linear2.weight", "swinViT.layers1.0.blocks.1.mlp.linear2.bias", "swinViT.layers1.0.downsample.reduction.weight", "swinViT.layers1.0.downsample.norm.weight", "swinViT.layers1.0.downsample.norm.bias", "swinViT.layers2.0.blocks.0.norm1.weight", "swinViT.layers2.0.blocks.0.norm1.bias", "swinViT.layers2.0.blocks.0.attn.relative_position_bias_table", "swinViT.layers2.0.blocks.0.attn.relative_position_index", "swinViT.layers2.0.blocks.0.attn.qkv.weight", "swinViT.layers2.0.blocks.0.attn.qkv.bias", "swinViT.layers2.0.blocks.0.attn.proj.weight", "swinViT.layers2.0.blocks.0.attn.proj.bias", "swinViT.layers2.0.blocks.0.norm2.weight", "swinViT.layers2.0.blocks.0.norm2.bias", "swinViT.layers2.0.blocks.0.mlp.linear1.weight", "swinViT.layers2.0.blocks.0.mlp.linear1.bias", "swinViT.layers2.0.blocks.0.mlp.linear2.weight", "swinViT.layers2.0.blocks.0.mlp.linear2.bias", "swinViT.layers2.0.blocks.1.norm1.weight", "swinViT.layers2.0.blocks.1.norm1.bias", "swinViT.layers2.0.blocks.1.attn.relative_position_bias_table", "swinViT.layers2.0.blocks.1.attn.relative_position_index", "swinViT.layers2.0.blocks.1.attn.qkv.weight", "swinViT.layers2.0.blocks.1.attn.qkv.bias", "swinViT.layers2.0.blocks.1.attn.proj.weight", "swinViT.layers2.0.blocks.1.attn.proj.bias", "swinViT.layers2.0.blocks.1.norm2.weight", "swinViT.layers2.0.blocks.1.norm2.bias", "swinViT.layers2.0.blocks.1.mlp.linear1.weight", "swinViT.layers2.0.blocks.1.mlp.linear1.bias", "swinViT.layers2.0.blocks.1.mlp.linear2.weight", "swinViT.layers2.0.blocks.1.mlp.linear2.bias", "swinViT.layers2.0.downsample.reduction.weight", "swinViT.layers2.0.downsample.norm.weight", "swinViT.layers2.0.downsample.norm.bias", "swinViT.layers3.0.blocks.0.norm1.weight", "swinViT.layers3.0.blocks.0.norm1.bias", "swinViT.layers3.0.blocks.0.attn.relative_position_bias_table", "swinViT.layers3.0.blocks.0.attn.relative_position_index", "swinViT.layers3.0.blocks.0.attn.qkv.weight", "swinViT.layers3.0.blocks.0.attn.qkv.bias", "swinViT.layers3.0.blocks.0.attn.proj.weight", "swinViT.layers3.0.blocks.0.attn.proj.bias", "swinViT.layers3.0.blocks.0.norm2.weight", "swinViT.layers3.0.blocks.0.norm2.bias", "swinViT.layers3.0.blocks.0.mlp.linear1.weight", "swinViT.layers3.0.blocks.0.mlp.linear1.bias", "swinViT.layers3.0.blocks.0.mlp.linear2.weight", "swinViT.layers3.0.blocks.0.mlp.linear2.bias", "swinViT.layers3.0.blocks.1.norm1.weight", "swinViT.layers3.0.blocks.1.norm1.bias", "swinViT.layers3.0.blocks.1.attn.relative_position_bias_table", "swinViT.layers3.0.blocks.1.attn.relative_position_index", "swinViT.layers3.0.blocks.1.attn.qkv.weight", "swinViT.layers3.0.blocks.1.attn.qkv.bias", "swinViT.layers3.0.blocks.1.attn.proj.weight", "swinViT.layers3.0.blocks.1.attn.proj.bias", "swinViT.layers3.0.blocks.1.norm2.weight", "swinViT.layers3.0.blocks.1.norm2.bias", "swinViT.layers3.0.blocks.1.mlp.linear1.weight", "swinViT.layers3.0.blocks.1.mlp.linear1.bias", "swinViT.layers3.0.blocks.1.mlp.linear2.weight", "swinViT.layers3.0.blocks.1.mlp.linear2.bias", "swinViT.layers3.0.downsample.reduction.weight", "swinViT.layers3.0.downsample.norm.weight", "swinViT.layers3.0.downsample.norm.bias", "swinViT.layers4.0.blocks.0.norm1.weight", "swinViT.layers4.0.blocks.0.norm1.bias", "swinViT.layers4.0.blocks.0.attn.relative_position_bias_table", "swinViT.layers4.0.blocks.0.attn.relative_position_index", "swinViT.layers4.0.blocks.0.attn.qkv.weight", "swinViT.layers4.0.blocks.0.attn.qkv.bias", "swinViT.layers4.0.blocks.0.attn.proj.weight", "swinViT.layers4.0.blocks.0.attn.proj.bias", "swinViT.layers4.0.blocks.0.norm2.weight", "swinViT.layers4.0.blocks.0.norm2.bias", "swinViT.layers4.0.blocks.0.mlp.linear1.weight", "swinViT.layers4.0.blocks.0.mlp.linear1.bias", "swinViT.layers4.0.blocks.0.mlp.linear2.weight", "swinViT.layers4.0.blocks.0.mlp.linear2.bias", "swinViT.layers4.0.blocks.1.norm1.weight", "swinViT.layers4.0.blocks.1.norm1.bias", "swinViT.layers4.0.blocks.1.attn.relative_position_bias_table", "swinViT.layers4.0.blocks.1.attn.relative_position_index", "swinViT.layers4.0.blocks.1.attn.qkv.weight", "swinViT.layers4.0.blocks.1.attn.qkv.bias", "swinViT.layers4.0.blocks.1.attn.proj.weight", "swinViT.layers4.0.blocks.1.attn.proj.bias", "swinViT.layers4.0.blocks.1.norm2.weight", "swinViT.layers4.0.blocks.1.norm2.bias", "swinViT.layers4.0.blocks.1.mlp.linear1.weight", "swinViT.layers4.0.blocks.1.mlp.linear1.bias", "swinViT.layers4.0.blocks.1.mlp.linear2.weight", "swinViT.layers4.0.blocks.1.mlp.linear2.bias", "swinViT.layers4.0.downsample.reduction.weight", "swinViT.layers4.0.downsample.norm.weight", "swinViT.layers4.0.downsample.norm.bias", "rotation_head.weight", "rotation_head.bias", "contrastive_head.weight", "contrastive_head.bias", "conv.0.weight", "conv.0.bias", "conv.4.weight", "conv.4.bias", "conv.8.weight", "conv.8.bias", "conv.12.weight", "conv.12.bias", "conv.16.weight", "conv.16.bias", "conv.20.weight", "conv.20.bias". 
	    Unexpected key(s) in state_dict: "module.patch_embed.proj.weight", "module.patch_embed.proj.bias", 
        - [Ok] from mine
---------- comments ----------------
1. validate on single GPU, not on multi.
2. - the trainig while loop is controled by args.num_steps & global_step, 
   i.e., iteration (within an epoch). Frequency of validation is also
   controlled by global_step.
   - args.epochs is only used to setup lr scheduler.   
3. no barrier like in BRATS script.
- before each training epoch
- before validation
4. train_sampler different than BRATS script
- train_sampler set up is different, no shuffle
- no set_epoch before each epoch
5. resume, diff than BRATS script
- doesn't save the best val performance, doesn't load best val
- doesn't load epoch, so need to put args.epochs > resumed_epoch?
6. scheduler.step() after each iteration/batch, BRATS do it after each epoch
[~ok] 7. warning
Detected call of `lr_scheduler.step()` before `optimizer.step()`. 
In PyTorch 1.1.0 and later, you should call them in the opposite order: 
`optimizer.step()` before `lr_scheduler.step()`.  
Failure to do this will result in PyTorch skipping the first value of 
the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
[ok] - grep scheduler.step -> only used once
[ok] - caused by amp (scaler) -> disenabled, warning gone
    - https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#adding-gradscaler
    # Scales loss.  Calls backward() on scaled loss to create scaled gradients.
    scaler.scale(loss).backward()

    # scaler.step() first unscales the gradients of the optimizer's assigned params.
    # If these gradients do not contain infs or NaNs, optimizer.step() is then called,
    # otherwise, optimizer.step() is skipped.
    scaler.step(opt)

    # Updates the scale for next iteration.
    scaler.update()

    - https://discuss.pytorch.org/t/userwarning-detected-call-of-lr-scheduler-step-before-optimizer-step-in-pytorch-1-1-0-and-later-you-should-call-them-in-the-opposite-order-optimizer-step-before-lr-scheduler-step/88295/5


[ok] 8. need barrier before validation?
- [ok] no barrier - output no problem
- with barrier


----------------------------------------------------------------------
DDP aggregate loss by default, no need to implement distributed_all_gather?
synchronize at loss?
each process loss different
[2] train: epoch 4/2, step_within_epoch 58/90, global_step 422/29999, loss 1.8776, time 1.39s
[1] train: epoch 4/2, step_within_epoch 58/90, global_step 422/29999, loss 1.8451, time 1.40s
[3] train: epoch 4/2, step_within_epoch 58/90, global_step 422/29999, loss 1.8137, time 1.42s
[0] train: epoch 4/2, step_within_epoch 58/90, global_step 422/29999, loss 2.4658, time 1.40s
[6] train: epoch 4/2, step_within_epoch 58/90, global_step 422/29999, loss 2.4245, time 1.40s
[5] train: epoch 4/2, step_within_epoch 58/90, global_step 422/29999, loss 1.9779, time 1.40s
[7] train: epoch 4/2, step_within_epoch 58/90, global_step 422/29999, loss 2.0713, time 1.40s
[4] train: epoch 4/2, step_within_epoch 58/90, global_step 422/29999, loss 1.4271, time 1.40s


cachedataset/samrtcache in DDP:
train_loader.sampler.set_epoch(epoch), cache whole dataset on each processes's CPU?
without, cache part of dataset on each processes's CPU?

checkpoint = {"epoch": model.epoch, "state_dict": model.state_dict(), "optimizer": optimizer.state_dict()}


[ok] `RuntimeError: Expected to mark a variable ready only once.` Details see below ====
https://discuss.pytorch.org/t/finding-the-cause-of-runtimeerror-expected-to-mark-a-variable-ready-only-once/124428
activation checkpoint? amp? -> checkpointing gradients is the cause!
- [ok] no --use_checkpoint, use --noamp -> works!
- [ok] no --use_checkpoint, no --noamp -> works!
- [ok] --use_checkpoint, no --noamp -> doesn't work!
=====
Error detected in torch::autograd::AccumulateGrad. Traceback of forward call that caused the error:
  File "/home/hju/run_monaicore/swinViT_pretrain/main.py", line 366, in <module>
    main()
  File "/home/hju/run_monaicore/swinViT_pretrain/main.py", line 319, in main
    model = DistributedDataParallel(model, device_ids=[args.local_rank])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 658, in __init__
    self._ddp_init_helper(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 724, in _ddp_init_helper
    self.reducer = dist.Reducer(
 (Triggered internally at  /opt/pytorch/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:102.)
======
Error detected in CheckpointFunctionBackward. Traceback of forward call that caused the error:
  File "/home/hju/run_monaicore/swinViT_pretrain/main.py", line 366, in <module>
    main()
  File "/home/hju/run_monaicore/swinViT_pretrain/main.py", line 339, in main
    global_step, loss, best_val = train(args, global_step, train_loader, best_val, scaler, count_epoch)
  File "/home/hju/run_monaicore/swinViT_pretrain/main.py", line 51, in train
    rot1_p, contrastive1_p, rec_x1 = model(x1_augment)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hju/run_monaicore/swinViT_pretrain/models/ssl_head.py", line 80, in forward
    x_out = self.swinViT(x.contiguous())[4]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/monai/monai/networks/nets/swin_unetr.py", line 1026, in forward
    x4 = self.layers4[0](x3.contiguous())
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/monai/monai/networks/nets/swin_unetr.py", line 885, in forward
    x = blk(x, attn_mask)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/monai/monai/networks/nets/swin_unetr.py", line 672, in forward
    x = x + checkpoint.checkpoint(self.forward_part2, x)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 235, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
 (Triggered internally at  /opt/pytorch/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:102.)
 ===========
 Traceback (most recent call last):
  File "/home/hju/run_monaicore/swinViT_pretrain/main.py", line 366, in <module>
Traceback (most recent call last):
  File "/home/hju/run_monaicore/swinViT_pretrain/main.py", line 366, in <module>
    main()
  File "/home/hju/run_monaicore/swinViT_pretrain/main.py", line 339, in main
    main()
  File "/home/hju/run_monaicore/swinViT_pretrain/main.py", line 339, in main
    global_step, loss, best_val = train(args, global_step, train_loader, best_val, scaler, count_epoch)
  File "/home/hju/run_monaicore/swinViT_pretrain/main.py", line 61, in train
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    global_step, loss, best_val = train(args, global_step, train_loader, best_val, scaler, count_epoch)
  File "/home/hju/run_monaicore/swinViT_pretrain/main.py", line 61, in train
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
  ============
RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
Parameter at index 113 with name swinViT.layers4.0.blocks.1.mlp.linear2.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
        torch.autograd.backward(outputs_with_grad, args_with_grad)torch.autograd.backward(outputs_with_grad, args_with_grad)


----------------
