/home/hju/monaicore_multigpu/unet_ddp
c0800a-s17.ufhpc
Thu Jun  9 11:48:55 EDT 2022
Primary node: c0800a-s17
Primary TCP port: 21186
Secondary nodes: 
Running "/home/hju/run_monaicore/multigpu/unet_ddp/unet_training_ddp.py" on each node...
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (484) bind mounts
/.singularity.d/env/10-docker2singularity.sh: line 2: uname: No such file or directory
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[0] generating synthetic data to ./testdata (this may take a while)
NCCL version 2.11.4+cuda11.4
[0] ---------- epoch 1/5[1] ---------- epoch 1/5

[2] ---------- epoch 1/5
[3] ---------- epoch 1/5
[3] 1/32, train_loss: 0.6683
[0] 1/32, train_loss: 0.6842
[2] 1/32, train_loss: 0.7440
[1] 1/32, train_loss: 0.6521
[3] 2/32, train_loss: 0.5461
[2] 2/32, train_loss: 0.6759
[0] 2/32, train_loss: 0.5904
[1] 2/32, train_loss: 0.5674
[3] 3/32, train_loss: 0.5674
[0] 3/32, train_loss: 0.5174
[1] 3/32, train_loss: 0.5469
[2] 3/32, train_loss: 0.5537
[3] 4/32, train_loss: 0.5220
[0] 4/32, train_loss: 0.5445
[2] 4/32, train_loss: 0.5583
[1] 4/32, train_loss: 0.5365
[3] 5/32, train_loss: 0.6034[2] 5/32, train_loss: 0.4755

[0] 5/32, train_loss: 0.5286
[1] 5/32, train_loss: 0.5504
[1] 6/32, train_loss: 0.5131
[3] 6/32, train_loss: 0.5039[0] 6/32, train_loss: 0.4775

[2] 6/32, train_loss: 0.4617
[2] 7/32, train_loss: 0.5025
[0] 7/32, train_loss: 0.4937
[3] 7/32, train_loss: 0.4922
[1] 7/32, train_loss: 0.5197
[3] 8/32, train_loss: 0.4981
[0] 8/32, train_loss: 0.4658
[2] 8/32, train_loss: 0.5673
[1] 8/32, train_loss: 0.5198
[1] epoch 1, average loss: 0.5507
[1] ---------- epoch 2/5
[3] epoch 1, average loss: 0.5502
[3] ---------- epoch 2/5
[2] epoch 1, average loss: 0.5674
[2] ---------- epoch 2/5
[0] epoch 1, average loss: 0.5377
[0] ---------- epoch 2/5
[0] 1/32, train_loss: 0.5029
[3] 1/32, train_loss: 0.5605
[2] 1/32, train_loss: 0.5078
[1] 1/32, train_loss: 0.5110
[3] 2/32, train_loss: 0.5180
[0] 2/32, train_loss: 0.4392[1] 2/32, train_loss: 0.4657

[2] 2/32, train_loss: 0.5203
[2] 3/32, train_loss: 0.4729
[0] 3/32, train_loss: 0.4792
[3] 3/32, train_loss: 0.5285[1] 3/32, train_loss: 0.4409

[2] 4/32, train_loss: 0.5604
[3] 4/32, train_loss: 0.4643
[1] 4/32, train_loss: 0.4857
[0] 4/32, train_loss: 0.4535
[2] 5/32, train_loss: 0.4666
[3] 5/32, train_loss: 0.6052
[0] 5/32, train_loss: 0.4780
[1] 5/32, train_loss: 0.4722
[3] 6/32, train_loss: 0.4104
[2] 6/32, train_loss: 0.4813
[1] 6/32, train_loss: 0.4803
[0] 6/32, train_loss: 0.5070
[0] 7/32, train_loss: 0.4775
[1] 7/32, train_loss: 0.4281
[2] 7/32, train_loss: 0.5255
[3] 7/32, train_loss: 0.4267
[3] 8/32, train_loss: 0.4000
[2] 8/32, train_loss: 0.4850
[1] 8/32, train_loss: 0.4009[0] 8/32, train_loss: 0.4654

[3] epoch 2, average loss: 0.4892
[3] ---------- epoch 3/5
[1] epoch 2, average loss: 0.4606
[1] ---------- epoch 3/5
[0] epoch 2, average loss: 0.4754
[0] ---------- epoch 3/5
[2] epoch 2, average loss: 0.5025
[2] ---------- epoch 3/5
[1] 1/32, train_loss: 0.4511
[3] 1/32, train_loss: 0.4953
[2] 1/32, train_loss: 0.4895
[0] 1/32, train_loss: 0.4514
[2] 2/32, train_loss: 0.5451
[0] 2/32, train_loss: 0.4496
[1] 2/32, train_loss: 0.3945
[3] 2/32, train_loss: 0.4352
[2] 3/32, train_loss: 0.4574
[0] 3/32, train_loss: 0.4863[1] 3/32, train_loss: 0.5063

[3] 3/32, train_loss: 0.4086
[3] 4/32, train_loss: 0.4235
[2] 4/32, train_loss: 0.5022
[1] 4/32, train_loss: 0.4521
[0] 4/32, train_loss: 0.4402
[3] 5/32, train_loss: 0.4372
[1] 5/32, train_loss: 0.4867
[0] 5/32, train_loss: 0.4186
[2] 5/32, train_loss: 0.5285
[3] 6/32, train_loss: 0.4366
[0] 6/32, train_loss: 0.4399
[2] 6/32, train_loss: 0.4302
[1] 6/32, train_loss: 0.4166
[3] 7/32, train_loss: 0.4515
[2] 7/32, train_loss: 0.4029
[1] 7/32, train_loss: 0.5376
[0] 7/32, train_loss: 0.4002
[3] 8/32, train_loss: 0.4489
[0] 8/32, train_loss: 0.4205
[1] 8/32, train_loss: 0.5414
[2] 8/32, train_loss: 0.4452
[3] epoch 3, average loss: 0.4421
[3] ---------- epoch 4/5
[1] epoch 3, average loss: 0.4733
[1] ---------- epoch 4/5
[0] epoch 3, average loss: 0.4383
[0] ---------- epoch 4/5
[2] epoch 3, average loss: 0.4751
[2] ---------- epoch 4/5
[1] 1/32, train_loss: 0.4333
[3] 1/32, train_loss: 0.4018
[0] 1/32, train_loss: 0.4520
[2] 1/32, train_loss: 0.5677
[0] 2/32, train_loss: 0.5423
[3] 2/32, train_loss: 0.4535
[1] 2/32, train_loss: 0.4253
[2] 2/32, train_loss: 0.3815
[1] 3/32, train_loss: 0.4121
[3] 3/32, train_loss: 0.6185
[0] 3/32, train_loss: 0.4508
[2] 3/32, train_loss: 0.3868
[2] 4/32, train_loss: 0.4182
[1] 4/32, train_loss: 0.4134
[3] 4/32, train_loss: 0.4528
[0] 4/32, train_loss: 0.3555
[3] 5/32, train_loss: 0.3848
[2] 5/32, train_loss: 0.4239
[0] 5/32, train_loss: 0.4193
[1] 5/32, train_loss: 0.4567
[2] 6/32, train_loss: 0.4358
[0] 6/32, train_loss: 0.4197
[3] 6/32, train_loss: 0.3763[1] 6/32, train_loss: 0.4529

[2] 7/32, train_loss: 0.3720
[0] 7/32, train_loss: 0.4027
[1] 7/32, train_loss: 0.4379
[3] 7/32, train_loss: 0.4137
[2] 8/32, train_loss: 0.4477
[3] 8/32, train_loss: 0.4486
[0] 8/32, train_loss: 0.4942
[1] 8/32, train_loss: 0.4062
[2] epoch 4, average loss: 0.4292
[2] ---------- epoch 5/5
[3] epoch 4, average loss: 0.4438
[3] ---------- epoch 5/5
[0] epoch 4, average loss: 0.4421[1] epoch 4, average loss: 0.4297
[0] ---------- epoch 5/5

[1] ---------- epoch 5/5
[3] 1/32, train_loss: 0.4044
[2] 1/32, train_loss: 0.4414
[1] 1/32, train_loss: 0.5190
[0] 1/32, train_loss: 0.4670
[3] 2/32, train_loss: 0.5714
[0] 2/32, train_loss: 0.3677
[1] 2/32, train_loss: 0.4161
[2] 2/32, train_loss: 0.4131
[0] 3/32, train_loss: 0.4590
[3] 3/32, train_loss: 0.3751
[2] 3/32, train_loss: 0.4060
[1] 3/32, train_loss: 0.3719
[0] 4/32, train_loss: 0.4133
[3] 4/32, train_loss: 0.4030
[2] 4/32, train_loss: 0.4087
[1] 4/32, train_loss: 0.4134
[3] 5/32, train_loss: 0.3954
[0] 5/32, train_loss: 0.4064
[1] 5/32, train_loss: 0.4739
[2] 5/32, train_loss: 0.4276
[0] 6/32, train_loss: 0.4235
[1] 6/32, train_loss: 0.5238
[3] 6/32, train_loss: 0.3730
[2] 6/32, train_loss: 0.3716
[3] 7/32, train_loss: 0.3969
[1] 7/32, train_loss: 0.4212
[0] 7/32, train_loss: 0.3689
[2] 7/32, train_loss: 0.4470
[2] 8/32, train_loss: 0.4622
[0] 8/32, train_loss: 0.4651
[1] 8/32, train_loss: 0.4552
[3] 8/32, train_loss: 0.3864
[0] epoch 5, average loss: 0.4213
[2] epoch 5, average loss: 0.4222
[0] train completed, epoch losses: [0.5377451255917549, 0.47535787895321846, 0.43833767995238304, 0.44205401465296745, 0.4213467389345169]
[2] train completed, epoch losses: [0.5673531591892242, 0.5024664476513863, 0.4751189425587654, 0.42920219898223877, 0.4222086928784847]
[1] epoch 5, average loss: 0.4493
[1] train completed, epoch losses: [0.550727590918541, 0.4605930373072624, 0.4732794389128685, 0.42971617728471756, 0.4493245556950569]
[3] epoch 5, average loss: 0.4132
[3] train completed, epoch losses: [0.5501730442047119, 0.4892209954559803, 0.44207802414894104, 0.44376668334007263, 0.41319410130381584]